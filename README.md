Team Cadence's submission to MEDIQA-Chat-2023 shared task.

# Citation
```
@inproceedings{mediqa-chat-2023, 
            author = {Ashwyn Sharma and David I. Feldman and Aneesh Jain}, 
            title = {Team Cadence at MEDIQA-Chat 2023: Generating, augmenting and summarizing clinical dialogue with large language models.},
            booktitle = {ACL-ClinicalNLP 2023},
            year = {2023}
            }
```

# How to setup this repo
```
# Setup Git LFS
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash

sudo apt-get install git-lfs


git lfs install 

git clone git@github.com:ashwyn/MEDIQA-Chat-2023-Cadence.git

# Without lfs pull, models will not be downloaded 
git lfs pull
```

# How to run this code
```
cd code
source ./install.sh
source ./activate.sh
bash ./decode_task{A,B,C}_run{1,2,3}.sh [input-csv-file]

bash ./decode_taskA_run1.sh ../data/MEDIQA-Chat-TestSets-March-15-2023/TaskA/taskA_testset4participants_inputConversations.csv

bash ./decode_taskB_run1.sh ../data/MEDIQA-Chat-TestSets-March-15-2023/TaskB/taskB_testset4participants_inputConversations.csv

bash ./decode_taskB_run2.sh ../data/MEDIQA-Chat-TestSets-March-15-2023/TaskB/taskB_testset4participants_inputConversations.csv

bash ./decode_taskC_run1.sh ../data/MEDIQA-Chat-TestSets-March-15-2023/TaskC/taskC_testset4participants_inputNotes.csv
```


Outputs are generated in the code/outputs directory with the name formats laid out in the code submission guidelines. 

This code has been tested on an AWS g4dn.12xlarge instance.


# Methods

- Classification: bart-large trained for summarization (TaskA and TaskB) was fine-tuned for classification.
- Summarization (TaskA):
    - Fine-tuned bart-large on samsum for 30 epochs. 
    - Fine-tuned above model on TaskA dataset.
    - Fine-tuned above model on TaskA + Augmented dataset (1k note samples from MIMIC-IV and their dialogues generated by TaskC model)
- Summarization (TaskB):
    - Fine-tuned bart-large on samsum for 30 epochs. 
    - Fine-tuned above model on TaskB dataset.
    - Fine-tuned above model on TaskB + Augmented dataset (1k note samples from MIMIC-IV and their dialogues generated by TaskC model)
    - Run2:
        - Novel method to handle long input : trained a "2-pass" summarizer that can summarize a mix of partial summary and rest of the dialogue.
        - Run1 model is used to generated a summary for the first half of the dialogue.
        - First half summaries and rest of the dialogue (second half of the dialogue) is then used to create a "new" dataset.
        - Bart-large fine-tuned on samsum for 30 epochs is then fine-tuned on the above "new" dataset.
- Dialogue generation:
    - Fine-tuned bart-large on "samsum-inverse" i.e. flipped the input and target labels.
    - Fine-tuned above model on TaskC + TaskB + TaskA dataset. 
- Other methods explored:
    - Encoder decoder with bioclinicalbert and gpt2
    - PPL rescoring of the "nbest" list 
    - Shallow fusion in beam search with a GPT2 fine-tuned on MIMIC-IV notes